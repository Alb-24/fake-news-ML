from abc import ABC
from torch.utils.data import Dataset, DataLoader
import torch
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from sklearn.model_selection import train_test_split
import gensim
from nltk.stem import WordNetLemmatizer
import nltk
import pandas as pd
import matplotlib.pyplot as plt
import os

nltk.download('wordnet')

# Create directory where store the DataSets
os.makedirs('DataSet', exist_ok=True)
# Select GPU, if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define test size, used to split the data
test_size = 0.3


# Custom Pytorch DataSet
class NewsDataSet(Dataset, ABC):
    def __init__(self, articles_subset, labels_subset):
        """
        Arguments:
            articles_subset: pandas Subset["content"] containing articles content
            labels_subset: pandas Subset["label"] containing labels of articles
        """
        self.txt = articles_subset
        self.labels = labels_subset
        self.max_words = self.get_max_words()

    def get_max_words(self):
        # return the max number of words in a column (i.e. in an article's content)
        length_of_the_content = self.txt["content"].str.split("\\s+")
        return length_of_the_content.str.len().max()

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        # return a single item content and label
        label = self.labels[idx]
        text = self.txt[idx]
        item = {"Text": text, "Class": label}
        return item


# Function to retur a token for the input (npy array, containing text)
def yield_tokens(np_array):
    """
    ravel() is needed to reshape the np_array from [[text1],[text2],[text3],...] to [text1, text2, text3,...],
    otherwise iteration is not possible
    """
    for article_text in np_array.ravel():
        yield tokenizer(article_text)


# This function applies preprocess to the articles content, returning a list of preprocessed words
def preprocess(article_content):
    article_words = []
    # Remove stopwards and words with length<3 (so it removes also punctuation)
    for word in gensim.utils.simple_preprocess(article_content):
        if word not in gensim.parsing.preprocessing.STOPWORDS and len(word) > 3:
            article_words.append(word)

    lemma_words = []
    wordnet_lemmatizer = WordNetLemmatizer()
    # Applying lemmatization (e.g. ["drinking, drinked"] --> ["drink, drink"]
    for word in article_words:
        noun = wordnet_lemmatizer.lemmatize(word, pos="n")
        verb = wordnet_lemmatizer.lemmatize(noun, pos="v")
        adjective = wordnet_lemmatizer.lemmatize(verb, pos="a")
        lemma_words.append(adjective)
    return lemma_words


# Function that works on a batch of samples, generated by DataLoader
def collate_batch(batch):
    label_list, text_list = [], [],
    for (_label, _text) in batch:
        label_list.append(label_pipeline(_label))
        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)
        text_list.append(processed_text)
    label_list = torch.tensor(label_list, dtype=torch.int64)
    text_list = torch.cat(text_list)
    return label_list.to(device), text_list.to(device)


def combine_datasets(csv_true_path, csv_fake_path):
    # If the out.csv, containing concateneted True and Fake news, is not created, then proceed to create it
    if not os.path.exists("DataSet/out.csv"):
        # Load True and Fake Datasets as dataframes (df)
        df_true = pd.read_csv(csv_true_path)
        df_fake = pd.read_csv(csv_fake_path)

        # Add a column to store the "fake" label (0=true news, 1=fake news)
        df_true['fake'] = 0
        df_true.head()

        df_fake['fake'] = 1
        df_fake.head()

        # Concatenate both datasets into one unique
        df = pd.concat([df_fake, df_true]).reset_index(drop=True)

        # Combine title and text of the articles,
        # as it is easier to manipulate them together
        df['content'] = df['title'] + ' ' + df['text']

        # Join the content words as a string
        df['content'] = df['content'].apply(lambda x: "".join(x))

        # Remove the column of articles' date, because it's useless
        df.drop(columns=['date'], inplace=True)

        print(df["content"][0])
        # Apply text preprocess, removing stopwords and lemmatizating
        df["content"] = df["content"].apply(lambda row: preprocess(article_content=row))
        print(df["content"][0])
        # Since preprocess returns a list of words, convert it into a string ["an","example"] --> ["an example"]
        df['content'] = df['content'].apply(" ".join)
        print(df["content"][0])

        # Save out.csv file
        df.to_csv('DataSet/out.csv')


# Combine the two datasets, producing the combined dataset
combine_datasets("True.csv", "Fake.csv")

# Retrieve the combined DataSet
df = pd.read_csv('DataSet/out.csv')


# Show plot and count of true and fake news
df["fake"].value_counts().plot(title="Number of Fake and True News", kind="bar", y="fake")
plt.show()
"""
 value.counts() returns
 fake
 1    23481 --> Fake News
 0    21417 --> True news
"""
print("Number of Fake News:", df["fake"].value_counts()[0])
print("Number of True News:", df["fake"].value_counts()[1])

df.to_csv("DataSet/out.csv")

dataSet_path = "DataSet/out.csv"
# Passing the pandas Dataframe to NewsDataSet PyTorch DataSet class
news_dataset = NewsDataSet(articles_subset=pd.read_csv(filepath_or_buffer=dataSet_path, usecols=["content"]),
                           labels_subset=pd.read_csv(filepath_or_buffer=dataSet_path, usecols=["fake"]))

# Obtain the features and the labels
X = news_dataset.txt.values
Y = news_dataset.labels.values

# Random split in test and train data
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, shuffle=True)

"""
# TESTING THE OUTPUTS
print("Train: ", x_train[0:2])
print("Label: ", y_train[0:2])
print("Test: ", x_test[0:2])
print("Label: ", y_test[0:2])
"""

# Initialize tokenizer with english language
tokenizer = get_tokenizer('basic_english')

# Create the vocabulary, containing the mapping word-token
vocab_train = build_vocab_from_iterator(yield_tokens(x_train))

# text pipeline converts article's content (ndarray converted to string, eliminating parentesis and apices ['...'])
# to a list of integers, based on the vocabolary mapping
text_pipeline = lambda x: vocab_train(tokenizer((str(x).lstrip("['").rstrip("']"))))
# label_pipeline instead will transform the label into an integer
label_pipeline = lambda x: int(x) - 1
"""
Example:
 print(text_pipeline("Donald Trump")) 
>> [75, 16]
 print(label_pipeline("75"))
>> 74
"""
print("Article: \n", x_train[0])
print("Tokenized as:\n", text_pipeline(x_train[0]))
# Create a data loader, using the collate batch function
dataloader = DataLoader(x_train, batch_size=8, shuffle=False, collate_fn=collate_batch)
