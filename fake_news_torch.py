from abc import ABC

import numpy as np
from torch.utils.data import Dataset, DataLoader
import torch
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from sklearn.model_selection import train_test_split
import pandas as pd
import matplotlib.pyplot as plt
import os

# Create directory where store the DataSets
os.makedirs('Dataset', exist_ok=True)
# Select GPU, if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define test size, used to split the data
test_size = 0.3


# Custom Pytorch Dataset
class NewsDataSet(Dataset, ABC):
    def __init__(self, csv_file):
        """
        Arguments:
            csv_file (string): Path to the out.csv file
        """
        self.txt = pd.read_csv(filepath_or_buffer=csv_file, usecols=["content"], sep=",")
        self.labels = pd.read_csv(filepath_or_buffer=csv_file, usecols=["fake"], sep=",")
        self.max_words = self.get_max_words()

    def get_max_words(self):
        # return the max number of words in a column (i.e. in an article's content)
        length_of_the_content = self.txt["content"].str.split("\\s+")
        return length_of_the_content.str.len().max()

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        # return a single item content and label
        label = self.labels[idx]
        text = self.txt[idx]
        item = {"Text": text, "Class": label}
        return item


# Function to retur a token for the input (npy array, containing text)
def yield_tokens(np_array):
    """
    ravel() is needed to reshape the np_array from [[text1],[text2],[text3],...] to [text1, text2, text3,...],
    otherwise iteration is not possible
    """
    for article_text in np_array.ravel():
        yield tokenizer(article_text)


# Function that works on a batch of samples, generated by DataLoader
def collate_batch(batch):
    label_list, text_list = [], [],
    for (_label, _text) in batch:
        label_list.append(label_pipeline(_label))
        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)
        text_list.append(processed_text)
    label_list = torch.tensor(label_list, dtype=torch.int64)
    text_list = torch.cat(text_list)
    return label_list.to(device), text_list.to(device)


def combine_datasets(csv_true_path, csv_fake_path):
    # If the out.csv, containing concateneted True and Fake news, is not created, then proceed to create it
    if not os.path.exists("DataSet/out.csv"):
        # Load True and Fake Datasets as dataframes (df)
        df_true = pd.read_csv(csv_true_path)
        df_fake = pd.read_csv(csv_fake_path)

        # Add a column to store the "fake" label (0=true news, 1=fake news)
        df_true['fake'] = 0
        df_true.head()

        df_fake['fake'] = 1
        df_fake.head()

        # Concatenate both datasets into one unique
        df = pd.concat([df_fake, df_true]).reset_index(drop=True)

        # Combine title and text of the articles,
        # as it is easier to manipulate them together
        df['content'] = df['title'] + ' ' + df['text']

        # Join the content words as a string
        df['content'] = df['content'].apply(lambda x: "".join(x))

        # Remove the column of articles' date, because it's useless
        df.drop(columns=['date'], inplace=True)

        # Save out.csv file
        df.to_csv('DataSet/out.csv')


# Combine the two datasets, producing the combined dataset
combine_datasets("True.csv", "Fake.csv")

# Retrieve the combined Dataset
df = pd.read_csv('DataSet/out.csv')
# Show plot and count of true and fake news
df["fake"].value_counts().plot(title="Number of Fake and True News", kind="bar", y="fake")
plt.show()
"""
 value.counts() returns
 fake
 1    23481 --> Fake News
 0    21417 --> True news
"""
print("Number of Fake News:", df["fake"].value_counts()[0])
print("Number of True News:", df["fake"].value_counts()[1])

# Passing the pandas Dataframe to NewsDataSet PyTorch Dataset class
news_dataset = NewsDataSet('DataSet/out.csv')

# Obtain the features and the labels
X = news_dataset.txt.values
Y = news_dataset.labels.values

# Random split in test and train data
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, shuffle=True)

"""
# TESTING THE OUTPUTS
print("Train: ", x_train[0:2])
print("Label: ", y_train[0:2])
print("Test: ", x_test[0:2])
print("Label: ", y_test[0:2])
"""

# Initialize tokenizer with english language
tokenizer = get_tokenizer('basic_english')

# Create the vocabulary, containing the mapping word-token
vocab_train = build_vocab_from_iterator(yield_tokens(x_train))

# text pipeline will convert the article's content (text string) to a list of integers, based on the vocabolary mapping
# label_pipeline instead will transform the label into an integer
text_pipeline = lambda x: vocab_train(tokenizer(x))
label_pipeline = lambda x: int(x) - 1
"""
Example:
 print(text_pipeline("Donald Trump")) 
>> [75, 16]
 print(label_pipeline("75"))
>> 74
"""
# Create a data loader, using the collate batch function
dataloader = DataLoader(x_train, batch_size=8, shuffle=False, collate_fn=collate_batch)
